{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as skp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "import math\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "from focal_loss import SparseCategoricalFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(r\"path\\filename\")\n",
    "train_y = np.load(r\"path\\filename\")\n",
    "\n",
    "\n",
    "test_x = np.load(r\"path\\filename\")\n",
    "test_y = np.load(r\"path\\filename\")\n",
    "\n",
    "valid_x = np.load(r\"path\\filename\")\n",
    "valid_y = np.load(r\"path\\filename\")\n",
    "\n",
    "print(f\"Train Data\")\n",
    "print(f\"train_x data shape = {train_x.shape} \\ttrain_x data type = {train_x.dtype}\")\n",
    "print(f\"train_y data shape = {train_y.shape} \\ttrain_y data type = {train_y.dtype}\\n\\n\")\n",
    "\n",
    "print(f\"Test Data\")\n",
    "print(f\"test_x data shape = {test_x.shape} \\t\\ttest_x data type = {test_x.dtype}\")\n",
    "print(f\"test_y data shape = {test_y.shape} \\t\\ttest_y data type = {test_y.dtype}\\n\\n\")\n",
    "\n",
    "print(f\"Validation Data\")\n",
    "print(f\"valid_x data shape = {valid_x.shape} \\tvalid_x data type = {valid_x.dtype}\")\n",
    "print(f\"valid_y data shape = {valid_y.shape} \\tvalid_y data type = {valid_y.dtype}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Visulazitaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,12])\n",
    "plt.subplot(3,2,1)\n",
    "plt.imshow(train_x[0], cmap='gray',vmin=0, vmax=0.3)\n",
    "plt.title(\"grayscale train image\")\n",
    "plt.subplot(3,2,2)\n",
    "plt.imshow(train_y[0],cmap='jet_r')\n",
    "plt.title(\"Segmented train image\")\n",
    "plt.subplot(3,2,3)\n",
    "plt.imshow(valid_x[0],cmap='gray',vmin=0, vmax=0.3)\n",
    "plt.title(\"grayscale valid image\")\n",
    "plt.subplot(3,2,4)\n",
    "plt.imshow(valid_y[0],cmap='jet_r')\n",
    "plt.title(\"Segmented valid image\")\n",
    "plt.subplot(3,2,5)\n",
    "plt.imshow(test_x[0],cmap='gray',vmin=0, vmax=0.3)\n",
    "plt.title(\"grayscale test image\")\n",
    "plt.subplot(3,2,6)\n",
    "plt.imshow(test_y[0],cmap='jet_r')\n",
    "plt.title(\"Segmented test image\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UResNet Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NetWork Structure\n",
    "\n",
    "input = layers.Input(shape=train_x[0, :, :, :].shape)\n",
    "\n",
    "# B1 - Encode\n",
    "x = layers.Conv2D(filters=64, kernel_size=(4, 4), strides=1, padding='same')(input)\n",
    "x = layers.BatchNormalization()(x)\n",
    "E1LC = layers.Activation('relu')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(E1LC)\n",
    "\n",
    "# B2 - Encode\n",
    "x = layers.Conv2D(filters=128, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "E2SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(4, 4), strides=1, padding='same')(E2SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "E2LC = layers.Add()([x, E2SC])\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(E2LC)\n",
    "\n",
    "# B3 - Encode\n",
    "x = layers.Conv2D(filters=256, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "E3SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(4, 4), strides=1, padding='same')(E3SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "E3LC = layers.Add()([x, E3SC])\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(E3LC)\n",
    "\n",
    "# B4 - Encode\n",
    "x = layers.Conv2D(filters=512, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "E4SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(4, 4), strides=1, padding='same')(E4SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "E4LC = layers.Add()([x, E4SC])\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(E4LC)\n",
    "\n",
    "# Middle\n",
    "x = layers.Conv2D(filters=1024, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "MidSC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=1024, kernel_size=(4, 4), strides=1, padding='same')(MidSC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=1024, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Add()([x, MidSC])\n",
    "x = layers.Conv2DTranspose(filters=1024, kernel_size=(4, 4), strides=2, padding='same', kernel_regularizer =tf.keras.regularizers.l2(l=0.00001))(x)\n",
    "\n",
    "# B4S - Decode\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.concatenate([x,E4LC], axis=3)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "D4SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(4, 4), strides=1, padding='same')(D4SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Add()([x, D4SC])\n",
    "x = layers.Conv2DTranspose(filters=512, kernel_size=(4, 4), strides=2, padding='same')(x)\n",
    "\n",
    "# B3S - Decode\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.concatenate([x,E3LC], axis=3)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "D3SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(4, 4), strides=1, padding='same')(D3SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Add()([x, D3SC])\n",
    "x = layers.Conv2DTranspose(filters=256, kernel_size=(4, 4), strides=2, padding='same')(x)\n",
    "\n",
    "# B2S - Decode\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.concatenate([x,E2LC], axis=3)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "D2SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(4, 4), strides=1, padding='same')(D2SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Add()([x, D2SC])\n",
    "x = layers.Conv2DTranspose(filters=128, kernel_size=(4, 4), strides=2, padding='same')(x)\n",
    "\n",
    "# B1S - Decode\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.concatenate([x,E1LC], axis=3)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding='same')(x)\n",
    "D1SC = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(4, 4), strides=1, padding='same')(D1SC)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Add()([x, D1SC])\n",
    "\n",
    "#outputs\n",
    "output = layers.Conv2D(3, 1, activation=\"softmax\", padding=\"same\")(x)\n",
    "autoencoder = keras.models.Model(input, output)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2))\n",
    "\n",
    "\n",
    "def PSNR(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"min\", verbose=2, patience=10\n",
    ")\n",
    "# ReduceLROnPlateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.8, patience=5, mode=\"min\", verbose=1, min_lr=0.000001\n",
    ")\n",
    "# ModelCheckpoint\n",
    "ch_point = tf.keras.callbacks.ModelCheckpoint(\n",
    "    dir_cur+\"\\path\\filename\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    save_best_only=\"True\",\n",
    ")\n",
    "\n",
    "# CSVLogger\n",
    "log_train = tf.keras.callbacks.CSVLogger(\n",
    "   dir_cur+\"\\path\\filename\",\n",
    "    separator=\",\",\n",
    "    append=False,\n",
    ")\n",
    "\n",
    "\n",
    "# lr scheduler\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.82\n",
    "    epochs_drop = 5\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "lr_sch = tf.keras.callbacks.LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "Epochs = 150\n",
    "BATCH_s = 16\n",
    "initial_learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "autoencoder.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=BATCH_s,\n",
    "    epochs=Epochs,\n",
    "    validation_data=(valid_x, valid_y),\n",
    "    validation_batch_size=BATCH_s,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop, lr_sch, ch_point, log_train],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = autoencoder.history.history[\"loss\"]\n",
    "Val_Loss = autoencoder.history.history[\"val_loss\"]\n",
    "accuracy = autoencoder.history.history[\"accuracy\"]\n",
    "Val_accuracy = autoencoder.history.history[\"val_accuracy\"]\n",
    "\n",
    "np.save(\n",
    "    dir_cur+\"\\path\\filename\", np.array(Loss)\n",
    ")\n",
    "np.save(\n",
    "    dir_cur+\"\\path\\filename\",\n",
    "    np.array(Val_Loss),\n",
    ")\n",
    "np.save(\n",
    "   dir_cur+\"\\path\\filename\", np.array(accuracy)\n",
    ")\n",
    "np.save(\n",
    "    dir_cur+\"\\path\\filename\",\n",
    "    np.array(Val_accuracy),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(Loss,c='#3300cc',linewidth=1.9,marker='o',label = 'Training loss (MSE)',linestyle = '-', markersize= 8, markerfacecolor = '#FFFFFF',mew=2,markevery=5)\n",
    "plt.plot(Val_Loss,'#cc3300',linewidth=1.9,marker='X',label = 'Validation loss (MSE) ',linestyle = '--', markersize= 8, markerfacecolor = '#FFFFFF',mew=2,markevery=5)\n",
    "plt.title(\"Model loss\")\n",
    "plt.legend(fontsize='large', frameon=False, ncol=1, loc='upper right')\n",
    "plt.xlabel('Epoch', fontsize = 11)\n",
    "plt.ylabel('MSE Loss' , fontsize = 11)\n",
    "plt.grid(color=\"black\", linestyle=\"-\", linewidth=0.2)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(accuracy,c='#3300cc',linewidth=1.9,marker='o',label = 'Training Acc (SSIM)',linestyle = '-', markersize= 8, markerfacecolor = '#FFFFFF',mew=2,markevery=5)\n",
    "plt.plot(Val_accuracy, '#cc3300',linewidth=1.9,marker='X',label = 'Validation Acc (SSIM) ',linestyle = '--', markersize= 8, markerfacecolor = '#FFFFFF',mew=2,markevery=5)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('SSIM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(fontsize='large', frameon=False, ncol=1, loc='best')\n",
    "plt.xlabel('Epoch', fontsize = 11)\n",
    "plt.ylabel('Accuracy' , fontsize = 11)\n",
    "#plt.yticks(np.arange(0,1.01,0.04))\n",
    "plt.ylim([0.8,1])\n",
    "plt.grid(color = 'k', linestyle = '-', linewidth = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model(\n",
    "    dir_cur+\"\\path\\filename\", custom_objects = {\"ssim_loss\":ssim_loss,\"PSNR\":PSNR}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(test_x, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(autoencoder.predict(test_x),axis=3)\n",
    "predictions = np.array(np.expand_dims(predictions,-1),dtype=np.float32)\n",
    "np.save(r\"path\\filename\",predictions)\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_metric = []\n",
    "for i in range(len(test_x)):\n",
    "    ssim_metric.append(ssim_loss(test_y[i], predictions[i]))\n",
    "print(np.max(ssim_metric))\n",
    "print(np.where(ssim_metric == np.max(ssim_metric)))\n",
    "ssim_res = [np.min(ssim_metric), np.max(ssim_metric), np.mean(ssim_metric)]\n",
    "\n",
    "file = open(\"filename.txt\", \"w+\")\n",
    "file.write(str(ssim_res))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSNR Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_metric = []\n",
    "for i in range(len(test_x)):\n",
    "    psnr_metric.append(PSNR(test_y[i], predictions[i]))\n",
    "print(np.max(psnr_metric))\n",
    "print(np.where(psnr_metric == np.max(psnr_metric)))\n",
    "psnr_res = [np.min(psnr_metric), np.max(psnr_metric), np.mean(psnr_metric)]\n",
    "\n",
    "file_1 = open(\"filename.txt\", \"w+\")\n",
    "file_1.write(str(psnr_res))\n",
    "file_1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Over Union (IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU_keras = tf.keras.metrics.MeanIoU (num_classes=3)\n",
    "IoU_keras.update_state(test_y,predictions)\n",
    "values = np.array(IoU_keras.get_weights()).reshape(3,3)\n",
    "print(values)\n",
    "\n",
    "print(f'Mean Iou =  {IoU_keras.result().numpy():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "class1_Iou = values[0,0]/(values[0,0]+values[0,1]+values[0,2]+values[1,0]+values[2,0])\n",
    "class2_Iou = values[1,1]/(values[1,1]+values[1,0]+values[1,2]+values[0,1]+values[2,1])\n",
    "class3_Iou = values[2,2]/(values[2,2]+values[2,0]+values[2,1]+values[0,2]+values[1,2])\n",
    "print(f'IoU for class1 is: {class1_Iou:.4f}')\n",
    "print(f'IoU for class2 is: {class2_Iou:.4f}')\n",
    "print(f'IoU for class3 is: {class3_Iou:.4f}')\n",
    "\n",
    "file_2 = open(\"filename.txt\",\"w+\")\n",
    "file_2.write(f'Mean Iou =  {IoU_keras.result().numpy():.4f}')\n",
    "file_2.write(f'\\nIoU for class1 is: {class1_Iou:.4f}')\n",
    "file_2.write(f'\\nIoU for class2 is: {class2_Iou:.4f}')\n",
    "file_2.write(f'\\nIoU for class3 is: {class3_Iou:.4f}')\n",
    "file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(classification_report(np.ravel(test_y), np.ravel(predictions)))\n",
    "rep = classification_report(np.ravel(test_y), np.ravel(predictions))\n",
    "file_3 = open(\"filename.txt\", \"w+\")\n",
    "file_3.write(rep)\n",
    "file_3.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf_gpu_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef9c4c2b268f770537868d34064166b52c05f1ac393d08d7bf642e7f9f43ca2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
